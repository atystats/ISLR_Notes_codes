{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10\n",
    "# Unsupervised Learning\n",
    "A set of statistical tools intended for the settings in which we have only a set of features $X_1, X_2,....,X_p$ measured on n observations.\n",
    "\n",
    "Our goal is not prediction but finding interesting things about the measurements on $X_1, X_2,....,X_p$. \n",
    "1. __PCA__ :- PCA is used for data visualization or data pre processing before supervised techniques are applied.\n",
    "2. __Clustering__ :- Broad class of methods for discovering unknown subgroups of data.\n",
    "\n",
    "## Principal Component Analysis\n",
    "PCA allows us to summarize data with a smaller number of representative variables that collectively explain most of the variability in the original set. We are finding the directions in feature space along which the original data are highly variable.\n",
    "\n",
    "* PCA refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data.\n",
    "\n",
    "__What are Principal Components?__\n",
    "\n",
    "To visualize a dataset with n > 2 dimension is not possible. We wants to find a low-dimensional representation of the data that captures as much information as possible.\n",
    "\n",
    "The idea is that each of the observation of size n lives in p-dimensional space but not all dimensions are equally important. Each dimension found by PCA is a linear combination of the p features.\n",
    "\n",
    "First principal component is the normalized linear combination of the features\n",
    "$$Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 +......+ \\phi_{p1}X_p$$ that has the largest variance. By normalized, we mean $\\sum_{j = 1}^{p}\\phi_{j1}^2 = 1$.\n",
    "\n",
    "$\\phi's$ are called loadings for the first principal component. Together, the loadings make up the principal component loading vector, $\\phi_1 = (\\phi_{11},\\phi_{21},....,\\phi_{p1})^T$. We constraints the loading, so that their sum of squares is equal to one. Since otherwise setting these elements to a large absolute value will result in high variance. \n",
    "\n",
    "First Component solves the optimization problem $$maximize_{\\phi_{11},\\phi_{21},....,\\phi_{p1}}{\\frac{1}{n}\\sum_{i=1}^n(\\sum_{j=1}^p\\phi_{j1}x_{ij})^2}$$ subject to $\\sum_{j = 1}^{p}\\phi_{j1}^2 = 1$.\n",
    "\n",
    "Since $\\frac{1}{n}\\sum x_{ij} = 0$ (normalized), the average of the $z_{11},z_{21},....,z_{n1}$ will be zero as well. Hence, the objective we are maximizing is just the sample variance of the n values of $z_{i1}$. \n",
    "\n",
    "$z_{11},z_{21},....,z_{n1}$ are score of first principal component.The optimization problem is solved via an eigen decomposition.\n",
    "\n",
    "* After the first principal component $Z_1$ of the feature has been determined, we can find second principal component $Z_2$. $Z_2$ is a linear combination of $X_1, X_2,....,X_p$ that has miximal variance out of all linear combination that are uncorrelated with $Z_1$.\n",
    "$$Z_{i2} = \\phi_{12}X_{i1} + \\phi_{22}X_{i2} +......+ \\phi_{p2}X_{ip}$$\n",
    "$\\phi_2$ = Second principal component loading vector. $\\phi_2 = (\\phi_{12},\\phi_{22},....,\\phi_{p2})^T$ and $\\phi_1$ and $\\phi_2$ are orthogonal vectors.\n",
    "\n",
    "Find $\\phi_2$ is the same optimization problem as $\\phi_1$ with additional constraints that $\\phi_2$ is orthogonal to $\\phi_1$. We can use these pricipal components to produce low-dimensional views of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
