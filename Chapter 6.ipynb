{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6\n",
    "\n",
    "# Linear Model Selection and Regularization\n",
    "\n",
    "Some ways in which linear model can be improved by replacing least squares fitting with some alternative fitting procedures. These alternative procedures will improve the prediction accuracy and model interpretibility.\n",
    "\n",
    "__1. Prediction Accuracy__ :- Least square method is supposed to have low bias if the true model is linear and also if data is big enough it will have low variance as well. But if:\n",
    "\n",
    "    (i). p > n, no least square solution exist.\n",
    "    (ii). if n is not much larger than p, estimates will have higher variability and the model might overfit.\n",
    "   By constraining ot shrinkage, we can reduce the variance with some negligible increase in bias.\n",
    "\n",
    "__2. Model Interpretibility__ :- By setting irrelevant variables to zero, we can increase interpretibility of the model. Least square is unlikely to do that.\n",
    "\n",
    "\n",
    "## 3 methods of model selection and regularization :-\n",
    "__1. Best Subset Selection__:- Identifying a subset of p predictors that we believe related to response.\n",
    "\n",
    "__2. Shrinkage__:- Fitting a model with all variables but they are shrunken towards zero relative to least squares. Shrinkage can also perform variable selection.\n",
    "\n",
    "__3. Dimension Reduction__:- Projecting p predictors into M dimensional subspace, where M < p. This is done by taking M linear combination of the p variables.\n",
    "\n",
    "## Subset Selection :-\n",
    "Methods of selecting subset of predictors.\n",
    "\n",
    "### 1. Best Subset Selection :-\n",
    "Fit a seperate regression model for each combination of p predictors. Several stages of Best subset selection.\n",
    "1. Start with $M_o$ i.e. a null model.\n",
    "2. For k = 1,2,...p, Fit all $p \\choose k$ models and pick the best among them and call them $M_0$ based on RSS or $R^2$.\n",
    "3. Select a single best model from $M_0, M_1,......,M_p$ using cross validated prediction error, $C_p$, AIC, BIC or adjusted $R^2$.\n",
    "\n",
    "Step 2 is reducing the problem from one of $2^p$ to one of (p+1) possible models. But as p increases, RSS decreases and $R^2$ increase so we will end up selecting model with maximum variable. We need to check performance of test data and hence come step 3.\n",
    "\n",
    "For classification, instead of RSS or $R^2$, we can use deviance.\n",
    "\n",
    "__Deviance__:- Negetive two times the maximized log-likelihood; smaller the deviance, better the fit. Deviance can be used for a broader class of models. \n",
    "\n",
    "__Drawbacks of Best Subset Selection__ :-\n",
    "1. The drawback is that it can be computationally expensive if p is large.\n",
    "2. When p is large, we have a big search space and hence more chances of selecting a model that is good at training but not on testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
